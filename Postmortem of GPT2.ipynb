{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d435c79f",
   "metadata": {},
   "source": [
    "# GPT2 Pretrained Model By HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb6e6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2605061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model_generation = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# model #uncomment this line and see, there are 12 blocks in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5bfc5d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of tokenizer:  50257\n",
      "Inputs:  {'input_ids': tensor([[ 5195, 20161,  1324,   829,   466,   407,  5594,   319,   262, 14256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "#the tokenizer, tokenize the input and convert into ids which further used as a input to model\n",
    "inputs = tokenizer(\"Why pineapples do not belong on the pizza\", return_tensors=\"pt\")\n",
    "print(\"length of tokenizer: \", len(tokenizer))\n",
    "print(\"Inputs: \" ,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b0c5a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs) #model produces two outputs, and we are only concernd about first output\n",
    "#when we give input as a argument to the model, it calls the forward function internally\n",
    "outputs[0].shape # output of the model is ={batch_size, sequence_size, embedding_size}\n",
    "#this outputs {outputs[0]} can be used as a input to language modeling head or \n",
    "#some other linear layers for different purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6b2f4",
   "metadata": {},
   "source": [
    "## 1. Embedding of the input by the GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0de34015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before unsqueeze operation shape of position_embed:  torch.Size([10, 768])\n",
      "After unsqueeze operation shape of position_embed:  torch.Size([1, 10, 768])\n",
      "\n",
      "final_input_embed.shape:  torch.Size([1, 10, 768])\n",
      "shape of output, generated after skipping the embedding layer:  torch.Size([1, 10, 768])\n",
      "shape of output, generated without skipping the embedding layer:  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "#we will see how embedding can be generated using this pretrained model\n",
    "#this function will produce the embedding without considering the position of the tokens in input\n",
    "input_embed = model.get_input_embeddings()(inputs['input_ids'])\n",
    "input_embed.shape\n",
    "#now lets take the position of tokens into consideration for producing the final embedding vectors\n",
    "position_embeddings = model.wpe.weight\n",
    "seq_len = input_embed.size(1)\n",
    "position_embed = position_embeddings[:seq_len]\n",
    "print('Before unsqueeze operation shape of position_embed: ', position_embed.shape)\n",
    "print('After unsqueeze operation shape of position_embed: ', position_embed.unsqueeze(0).shape)\n",
    "\n",
    "final_input_embed = position_embed.unsqueeze(0) + input_embed\n",
    "#unsqueeze operation= this function increase the dimension, we do this to make it compatible with the shape of \n",
    "#token embed\n",
    "print(\"\\nfinal_input_embed.shape: \", final_input_embed.shape)\n",
    "\n",
    "#we can also skip the embedding layer of the gpt2\n",
    "#lets say, the embedding vector that we derived, received from some other source or model \n",
    "#we can use this as a input to gpt2 model by skipping the embedding layer\n",
    "outputs_ = model(inputs_embeds = final_input_embed)\n",
    "#output from the gpt2 model, not skipping the embedding layer\n",
    "outputs = model(input_ids=inputs['input_ids'])\n",
    "print(\"shape of output, generated after skipping the embedding layer: \", outputs_[0].shape)\n",
    "print(\"shape of output, generated without skipping the embedding layer: \", outputs[0].shape)\n",
    "#to skip the embedding layer, onhe need to generate the embedding vectors from some other model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bdee0",
   "metadata": {},
   "source": [
    "## 2. Output from the every hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26d7e353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape of the final hidden layer: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "#since there are 12 hidden layers in GPT2, we can get the output from every hidden layer and \n",
    "#can use this output to feed some other user defined models\n",
    "\n",
    "for i in range(12):\n",
    "    if (i == 0):\n",
    "        out = model.h[i](final_input_embed) #output from the first hidden layer\n",
    "    else:\n",
    "        out = model.h[i](out[0]) #output from the previous layer, used as a input to next layer\n",
    "print(\"output shape of the final hidden layer:\", out[0].shape)\n",
    "#this output can be used to feed other linear layers, for some other purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd5cc1",
   "metadata": {},
   "source": [
    "## 3. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d96c29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why pineapples do not belong on the pizza menu, they are not'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contrastive decoding\n",
    "#generating 15 tokens\n",
    "#generate is a predefined function by transformer library used to generate text, if model is a generative model\n",
    "#Based on the parameter given to generate function, it uses different sampling/decoding methods like greedy,\n",
    "#nucleus, or contrastive \n",
    "output_generated = model_generation.generate(input_ids=inputs['input_ids'], max_length=15, penalty_alpha=0.6,\n",
    "                                             top_k=6)\n",
    "tokenizer.decode(output_generated[0]) \n",
    "#decoding the generated output from the Langugage modeling Head of the GPT2 model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
